{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#입력변수:2개, 출력:3가지 종류\n",
    "\n",
    "\n",
    "# '기타(o)'분류기 :hf= Wo1x1+Wo2x2+b1\n",
    "# '포유류(m)'분류기:hf=Wm1x1+Wm2x2+b2\n",
    "# '조류(b)'분류기:hf=Wb1x1+Wb2x2+b3\n",
    "#  w변수가 6개 필요함 => 입력변수:2 * 출력변수:3 = 6\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#[털(x1), 날개(x2)]\n",
    "x_data = np.array(\n",
    "    [[0, 0], \n",
    "     [1, 0], \n",
    "     [1, 1], \n",
    "     [0, 0], \n",
    "     [0, 0], \n",
    "     [0, 1]])\n",
    "# [기타, 포유류, 조류]\n",
    "y_data = np.array([ #   [0 1 2 0 0 2]\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "         # [입력특성개수, 출력레이블개수] => [2, 3]\n",
    "w=tf.Variable(tf.random_normal([2,3]))\n",
    "b=tf.Variable(tf.zeros([3])) #분류기 개수만큼 존재\n",
    "x=tf.placeholder(tf.float32)\n",
    "y=tf.placeholder(tf.float32)\n",
    "\n",
    "hf= tf.add(tf.matmul(x,w),b) #x:(6,2), w:(2,3), xw:(6,3), b:(3,)\n",
    "hf=tf.nn.relu(hf)\n",
    "model=tf.nn.softmax(hf) #[3, 2, 1.5] => [0.5, 0.3, 0.2]\n",
    "cost=tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(0.01)\n",
    "train=optimizer.minimize(cost)\n",
    "#sigmoid(입력값을 0~1로 변환), relu함수(음수->0, 양수->양수값 그대로 출력)\n",
    "#ex) relu(-3) => 0, relu(5) =>5\n",
    "#ex) sigmoid(-3) => 0.1, sigmoid(5) => 0.8\n",
    "\n",
    "#0.1 * 0.1 * 0.1*...* 0.00001 => 0에 가까워\n",
    "#h(x)->sig->0~1->h(x)->sig->0~1->...->0 이 되기도 함 -> 신호손실\n",
    "#h(x)->relu ->h(x)->relu->h(x)->...->sigmoid()->0~1사이로 출력\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.261794\n",
      "20 1.2552612\n",
      "30 1.2488188\n",
      "40 1.242465\n",
      "50 1.2361985\n",
      "60 1.2300181\n",
      "70 1.2239225\n",
      "80 1.2179102\n",
      "90 1.2119802\n",
      "100 1.2061312\n"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    sess.run(train, feed_dict={x:x_data, y:y_data})\n",
    "    if(step+1)%10==0:\n",
    "        print(step+1, sess.run(cost, feed_dict={x:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 1 1 0 0 0]\n",
      "실제값: [0 1 2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "#결과 확인\n",
    "#[[0 1 0 ] [0 0 1]] => argmax => [1 2]\n",
    "prediction=tf.argmax(model,1) #[0.5, 0.3, 0.2]\n",
    "target=tf.argmax(y,1) #1은 행 단위로 이동해가면서 최대값을 찾음,0:\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={x:x_data}))\n",
    "print(\"실제값:\", sess.run(target, feed_dict={y:y_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:66.67\n"
     ]
    }
   ],
   "source": [
    "is_correct=tf.equal(prediction, target)\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct, tf.float32)) #True=>1, False=>0\n",
    "print(\"정확도:%.2f\" % sess.run(accuracy*100, feed_dict={x:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "nb_classes=10 #0~9\n",
    "x=tf.placeholder(tf.float32, shape=[None,784]) #784픽셀 =28*28이미지\n",
    "y=tf.placeholder(tf.float32, shape=[None,nb_classes]) \n",
    "w=tf.Variable(tf.random_normal([784,nb_classes]))\n",
    "b=tf.Variable(tf.random_normal([nb_classes]))\n",
    "     #                                    0    1    2     3     9\n",
    "hf= tf.nn.softmax(tf.matmul(x,w)+b ) #[0.01  0.05  0.1  0.4 ...0.01]\n",
    "cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf),1))\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf => [0.01  0.05  0.1  0.4 ...0.01]=>tf.argmax(hf,1)=> 3(가장 큰값 index)\n",
    "#y => [0 0 0 1 0 0 0 0 0 0] => tf.argmax(y,1) => 3\n",
    "#tf.equal => True/False => tf.cast(float) => 1 / 0\n",
    "iscorrect=tf.equal(tf.argmax(hf,1), tf.argmax(y,1))\n",
    "#실제값 2, 예측 2 => True\n",
    "accuracy=tf.reduce_mean(tf.cast(iscorrect, tf.float32))\n",
    "#1 0 1 1 1 1 0 1 1 1 1 ... 1 => accuracy : 맞은 갯수 / 전체 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에폭:    1 cost= 2.733399140\n",
      "에폭:    2 cost= 1.119548826\n",
      "에폭:    3 cost= 0.891142139\n",
      "에폭:    4 cost= 0.776757397\n",
      "에폭:    5 cost= 0.703182789\n",
      "에폭:    6 cost= 0.651897680\n",
      "에폭:    7 cost= 0.612597745\n",
      "에폭:    8 cost= 0.581298672\n",
      "에폭:    9 cost= 0.556303274\n",
      "에폭:   10 cost= 0.535545765\n",
      "에폭:   11 cost= 0.517683559\n",
      "에폭:   12 cost= 0.502425042\n",
      "에폭:   13 cost= 0.488461044\n",
      "에폭:   14 cost= 0.476381348\n",
      "에폭:   15 cost= 0.465343788\n",
      "학습종료\n",
      "정확도: 0.8931\n",
      "우리 모델 예측: [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:  #sess=tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(15): #15번 반복\n",
    "        costavr=0\n",
    "        totalbatch=int(mnist.train.num_examples/100) #5.5만개이미지/100 => 550\n",
    "        for i in range(totalbatch): #550번 반복, 한번에 100개씩 트레이닝하면 \n",
    "            batchx, batchy=mnist.train.next_batch(100)#한번에 100개의 이미지를 읽어들임(x,y)\n",
    "            cv, _=sess.run([cost, train], feed_dict={x:batchx, y:batchy})\n",
    "            costavr += cv/totalbatch #cost의 평균\n",
    "        print(\"에폭:\", \"%4d\"%(epoch+1), \"cost=\", \"{:.9f}\".format(costavr))\n",
    "    print(\"학습종료\")    \n",
    "    print(\"정확도:\", accuracy.eval(session=sess, feed_dict={\n",
    "        x:mnist.test.images, y:mnist.test.labels\n",
    "    }))\n",
    "    r=random.randint(0, mnist.test.num_examples-1)\n",
    "    print(\"우리 모델 예측:\",\n",
    "          sess.run(tf.argmax(hf,1), \n",
    "          feed_dict={x:mnist.test.images[r:r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples\n",
    "batchx, batchy=mnist.train.next_batch(1)\n",
    "batchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c110bf342607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"우리 모델 예측:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1073\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1075\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1076\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "r=random.randint(0, mnist.test.num_examples-1)\n",
    "print(\"우리 모델 예측:\",sess.run(tf.argmax(hf,1), feed_dict={x:mnist.test.images[r:r+1]}))\n",
    "plt.imshow(mnist.test.images[r:r+1].reshape(28,28))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW:단어의 출현 빈도를 나타내는 방법\n",
    "# -단어에 index\n",
    "# -index에 해당되는 단어의 등장 횟수를 벡터로 기록\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(\"가\", \"나\", \"가나다라가\") #\"가나다라\"에서 \"가\"를 \"나\"로 변경\n",
    "re.sub(\"가\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "token=re.sub(\"\\.\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No matching overloads found for tokenize in find. at native\\common\\jp_method.cpp:127",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-8be1b3eb2e47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#형태소분석 -> 토큰화\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mmorphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mphrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No matching overloads found for tokenize in find. at native\\common\\jp_method.cpp:127"
     ]
    }
   ],
   "source": [
    "token=okt.morphs(token) #형태소분석 -> 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2index={}\n",
    "bow=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voc in token:\n",
    "    if voc not in word2index.keys():\n",
    "        word2index[voc]=len(word2index)\n",
    "        bow.insert(len(word2index)-1, 1)\n",
    "    else:\n",
    "        index=word2index.get(voc)\n",
    "        bow[index]=bow[index]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(word2index) #각 형태소마다 인덱스를 부\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "#countervectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus=['you know I want your love. because I love you.']\n",
    "vector=CountVectorizer() #코퍼스에서 각 단어의 빈도수 조사\n",
    "#ex) 파이썬 자바 연필 -> 프로그래밍 코퍼스에 해당되지 않는것은? 연필\n",
    "#코퍼스 : 특정 분야의 해당되는 단어들의 집합\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "#   0 1 2 3 4 5 \n",
    "# [[1 1 2 1 2 1]] => 각 index위치에 해당되는 단어의 빈도\n",
    "#'you know I want your love. because I love you.'\n",
    "#'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vector=CountVectorizer(stop_words=['a','an','the', 'is','not'])\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer에 등록된 불용어 제거\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vector=CountVectorizer(stop_words=\"english\")\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "#nltk에 등록된 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "sw=stopwords.words(\"english\")\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vector=CountVectorizer(stop_words=sw)\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDM:단어 문서 행렬, 여러 문서에 등장하는 각 단어의 빈도를 나타낸 행렬\n",
    "# EX)\n",
    "# 문서1 : 자고 싶은 나\n",
    "# 문서2 : 자고 싶은 우리\n",
    "# 문서3 : 졸리고 졸린 우리\n",
    "# 문서4 : 저는 계속 졸려요\n",
    "\n",
    "#TDM예시\n",
    "#       자고  싶은 나  우리 졸리고 졸린 ...졸려요\n",
    "# 문서1  1     1   1   0     0      0  ...  0\n",
    "# ...\n",
    "# 문서4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-73-31ee0ceea1b2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-73-31ee0ceea1b2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tf:특정 문서에서 단어가 등장한 횟수\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tf:특정 문서에서 단어가 등장한 횟수\n",
    "df:특정 단어가 등장한 문서의 수\n",
    "문서1, 문서2, 문서3 중에서 '우리' 단어가 문서1에서 5번등장, 문서3에서 2번등장\n",
    "'우리' 단어의 df? 2\n",
    "idf = log(n / (1+df)), n:문서의 수(3)\n",
    "# 문서1 : 자고 싶은 나\n",
    "# 문서2 : 자고 싶은 우리\n",
    "# 문서3 : 졸리고 졸린 우리\n",
    "# 문서4 : 저는 계속 졸려요\n",
    "'자고' IDF?   log(4 / (1+2)) => log(4/3) =>0.xxx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "corpus=[\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'What should I do'    \n",
    "]\n",
    "vect=CountVectorizer()\n",
    "print(vect.fit_transform(corpus).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "tfidfv=TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.fit_transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#자연어처리 -> 주제어 단어 추출(TFIDF)\n",
    "\n",
    "#두 문서간 유사도 조사? 코사인 유사도:두 벡터 간 코사인 각도 -> 벡터 유사도"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
